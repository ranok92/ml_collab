{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import os\n",
    "import json\n",
    "\n",
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_OMCS_file(filename, language='en', save_filename=None):\n",
    "    '''\n",
    "    Given the filename and the language retrieves the sentences from the\n",
    "    provided language and stores it in another file.\n",
    "    input:\n",
    "        filename : string containing the name of the file\n",
    "        language : string containing the name of the language\n",
    "                   to look for.\n",
    "    \n",
    "    output: List of sentences from the OMCS text file belonging \n",
    "            to that particular language.\n",
    "    '''\n",
    "    data_list_language = []\n",
    "    \n",
    "    assert os.path.isfile(filename), 'File does not exist!'\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "         data_list = f.readlines()\n",
    "   \n",
    "    for data_line in data_list:\n",
    "        try:\n",
    "            if data_line.split('\\t')[4]==language:\n",
    "                data_list_language.append(data_line.split('\\t')[1])\n",
    "        except:\n",
    "            print('Found a faulty sentence :', data_line)\n",
    "            \n",
    "    if save_filename:\n",
    "        \n",
    "        with open(save_filename, 'w') as fp:\n",
    "            json.dump(data_list_language, fp)\n",
    "            \n",
    "    return data_list_language\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a faulty sentence : 1531350\tUm(a) golfinho Ã© usado(a) para brinc\n",
      "\n",
      "Found a faulty sentence : 1194903\tSome people like to check their e-mail first thing in the morning.\n",
      "\n",
      "Found a faulty sentence : 1194907\tPeople take showers in the morning.\n",
      "\n",
      "Found a faulty sentence : 1194921\tPeople eat cereal for breakfast.\n",
      "\n",
      "Found a faulty sentence : 1194929\tPeople can travel from work to home on a bicycle.\n",
      "\n",
      "Found a faulty sentence : 1194960\tPaper clips hold sheets of paper together.\n",
      "\n",
      "Found a faulty sentence : 1044642\tThe statement \"Telescopes make things look larger.\" is true because Refracting telescopes use lenses to gather and bend\n",
      "\n",
      "Found a faulty sentence : (898159 rows)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = read_OMCS_file('./data/omcs-sentences-free.txt', save_filename='english_sentences.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('english_sentences.txt', 'r') as fp:\n",
    "    en_data = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sometimes RB advmod causes VERB\n",
      "lightning NN nsubj causes VERB\n",
      "causes VBZ ROOT causes VERB\n",
      "electricity NN compound shortouts NOUN\n",
      "shortouts NNS dobj causes VERB\n"
     ]
    }
   ],
   "source": [
    "#printing token information using spacy\n",
    "sentence = en_data[121]\n",
    "tag_info = nlp(sentence)\n",
    "for token in tag_info:\n",
    "    print(token.text, token.tag_, token.dep_, token.head.text, token.head.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lightning  |  lightning  |  nsubj  |  causes\n",
      "electricity shortouts  |  shortouts  |  dobj  |  causes\n"
     ]
    }
   ],
   "source": [
    "#getting noun chunks using spacy\n",
    "doc = nlp(sentence)\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, \" | \", chunk.root.text, \" | \", chunk.root.dep_, \" | \",\n",
    "            chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the  |  det  |  film  |  NOUN  |  []\n",
      "film  |  nsubj  |  had  |  AUX  |  ['the']\n",
      "had  |  ROOT  |  had  |  AUX  |  ['film', 'patents', '.']\n",
      "200  |  nummod  |  patents  |  NOUN  |  []\n",
      "patents  |  dobj  |  had  |  AUX  |  ['200']\n",
      ".  |  punct  |  had  |  AUX  |  []\n"
     ]
    }
   ],
   "source": [
    "#navigating the parse tree\n",
    "sentence = \"the film had 200 patents.\"\n",
    "doc = nlp(sentence)\n",
    "for token in doc:\n",
    "    print(token.text,\" | \",  token.dep_,\" | \",  token.head.text,\" | \",  token.head.pos_,\" | \", \n",
    "            [child.text for child in token.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for getting the entities and relations from the tutorial\n",
    "\n",
    "def get_entities(sent):\n",
    "  ## chunk 1\n",
    "  ent1 = \"\"\n",
    "  ent2 = \"\"\n",
    "\n",
    "  prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\n",
    "  prv_tok_text = \"\"   # previous token in the sentence\n",
    "\n",
    "  prefix = \"\"\n",
    "  modifier = \"\"\n",
    "\n",
    "  #############################################################\n",
    "  \n",
    "  for tok in nlp(sent):\n",
    "    ## chunk 2\n",
    "    # if token is a punctuation mark then move on to the next token\n",
    "    if tok.dep_ != \"punct\":\n",
    "      # check: token is a compound word or not\n",
    "      if tok.dep_ == \"compound\":\n",
    "        prefix = tok.text\n",
    "        # if the previous word was also a 'compound' then add the current word to it\n",
    "        if prv_tok_dep == \"compound\":\n",
    "          prefix = prv_tok_text + \" \"+ tok.text\n",
    "      \n",
    "      # check: token is a modifier or not\n",
    "      if tok.dep_.endswith(\"mod\") == True:\n",
    "        modifier = tok.text\n",
    "        # if the previous word was also a 'compound' then add the current word to it\n",
    "        if prv_tok_dep == \"compound\":\n",
    "          modifier = prv_tok_text + \" \"+ tok.text\n",
    "      \n",
    "      ## chunk 3\n",
    "      if tok.dep_.find(\"subj\") == True:\n",
    "        ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n",
    "        prefix = \"\"\n",
    "        modifier = \"\"\n",
    "        prv_tok_dep = \"\"\n",
    "        prv_tok_text = \"\"      \n",
    "\n",
    "      ## chunk 4\n",
    "      if tok.dep_.find(\"obj\") == True:\n",
    "        ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n",
    "        \n",
    "      ## chunk 5  \n",
    "      # update variables\n",
    "      prv_tok_dep = tok.dep_\n",
    "      prv_tok_text = tok.text\n",
    "  #############################################################\n",
    "\n",
    "  return [ent1.strip(), ent2.strip()]\n",
    "\n",
    "\n",
    "def get_relation(sent):\n",
    "\n",
    "  doc = nlp(sent)\n",
    "\n",
    "  # Matcher class object \n",
    "  matcher = Matcher(nlp.vocab)\n",
    "\n",
    "  #define the pattern \n",
    "  pattern = [{'DEP':'ROOT'}, \n",
    "            {'DEP':'prep','OP':\"?\"},\n",
    "            {'DEP':'agent','OP':\"?\"},  \n",
    "            {'POS':'ADJ','OP':\"?\"}] \n",
    "\n",
    "  matcher.add(\"matching_1\", None, pattern) \n",
    "\n",
    "  matches = matcher(doc)\n",
    "  k = len(matches) - 1\n",
    "\n",
    "  span = doc[matches[k][1]:matches[k][2]] \n",
    "\n",
    "  return(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my code for extracting entities and relations\n",
    "\n",
    "    \n",
    "def check_verb_subject_in_clause(clause_token_list):\n",
    "    '''\n",
    "    Given a clause/sentence checks if the sentence contains\n",
    "    a subject and a verb.\n",
    "    input:\n",
    "        clause : a string containing a sentence\n",
    "        \n",
    "    output:\n",
    "        contains : A boolean flag indicating the presence \n",
    "                   of a verb and a subject in the sentence.\n",
    "    '''\n",
    "    pos_list = [\"VERB\", 'AUX']\n",
    "    dep_list = [\"ROOT\", 'aux']\n",
    "    contains = False\n",
    "    contains_verb = False\n",
    "    contains_sub = False\n",
    "    for token in clause_token_list:\n",
    "        #print(\"token : {} | pos : {} | dep : {}\".format(token.text, token.pos_, token.dep_))\n",
    "        if token.dep_.find(\"subj\") == True:\n",
    "            contains_sub = True\n",
    "        if token.pos_ in pos_list or token.dep_ in dep_list:\n",
    "            contains_verb = True\n",
    "        #print(\"Contains verb : {}, contains sub : {}\".format(contains_verb, contains_sub))\n",
    "    if contains_verb and contains_sub:\n",
    "        contains = True\n",
    "    return contains\n",
    "    \n",
    "\n",
    "        \n",
    "def get_subtree_excluding_subclause(node):\n",
    "    '''\n",
    "    Given the node, returns the subtree rooted at that node:\n",
    "    ideally to extract the subject or object subtree.\n",
    "    '''\n",
    "    black_list = [\"det\", \"punct\", \"conj\", \"cconj\"]\n",
    "    clausal_list = [\"ROOT\", \"csubj\", 'ccomp', \"prep\"\n",
    "                    'advcl', 'acl', 'conj',\n",
    "                   'relcl']\n",
    "    entity = []\n",
    "    entity_string = \"\"\n",
    "    for left in node.lefts:\n",
    "        \n",
    "        subtree_tokens, subtree_string, dep = get_subtree_excluding_subclause(left)\n",
    "        #if dep in clausal_list:\n",
    "            #print(\"From inside the function :\\n The token : {}\\n The subtree : {}\".format(left,subtree_tokens))\n",
    "            #print(\"Qualify for clause : \",check_verb_subject_in_clause(subtree_tokens))\n",
    "        \n",
    "        #add the tokens if dep is not in the clausal_list\n",
    "        if dep not in clausal_list: \n",
    "            for tok in subtree_tokens:\n",
    "                entity.append(tok)\n",
    "        else:\n",
    "        #even if it is in the clausal_list, add it if it does not have a verb\n",
    "        #and a subject\n",
    "            if not check_verb_subject_in_clause(subtree_tokens):\n",
    "                for tok in subtree_tokens:\n",
    "                    entity.append(tok)\n",
    "            \n",
    "    entity.append(node)\n",
    "    \n",
    "    for right in node.rights:\n",
    "        \n",
    "        subtree_tokens, subtree_string, dep = get_subtree_excluding_subclause(right)\n",
    "        #add the tokens if dep is not in the clausal_list\n",
    "        if dep not in clausal_list: \n",
    "            for tok in subtree_tokens:\n",
    "                entity.append(tok)\n",
    "        else:\n",
    "        #even if it is in the clausal_list, add it if it does not have a verb\n",
    "        #and a subject\n",
    "            if not check_verb_subject_in_clause(subtree_tokens):\n",
    "                for tok in subtree_tokens:\n",
    "                    entity.append(tok)\n",
    "                    \n",
    "    for value in entity:\n",
    "        entity_string += value.text\n",
    "        entity_string += \" \"\n",
    "        \n",
    "    entity_string = entity_string.strip()\n",
    "    return entity, entity_string, node.dep_\n",
    "    \n",
    "def extract_clauses(sent):\n",
    "    '''\n",
    "    Given a sentence, will break the sentence into \n",
    "    different clauses\n",
    "    '''\n",
    "    doc = nlp(sent)\n",
    "    #the idea is to find one of the dep_ from the following \n",
    "    #list \n",
    "    '''\n",
    "    [ parataxis, \n",
    "      conj - conjunct,\n",
    "      ccomp - clausal complement,\n",
    "      advcl - adverbal complement,\n",
    "      relcl - relative clause modifier,\n",
    "      csubj - clausal subject\n",
    "      xcomp - open clausal complement]\n",
    "    '''\n",
    "    clause_dep_list = [\"ROOT\", \"csubj\", 'ccomp', \"prep\",\n",
    "                       'advcl', 'acl', 'conj',\n",
    "                       'relcl']\n",
    "    \n",
    "    clause_list = []\n",
    "    for token in doc:\n",
    "\n",
    "        if token.dep_ in clause_dep_list:\n",
    "            \n",
    "            subclause, _, _ = get_subtree_excluding_subclause(token)\n",
    "#             check if the subclause contains a verb and a subject\n",
    "#             if not then it does not qualify as a clause\n",
    "#             print(\"Token :\", token)\n",
    "#             print(\"The subclause :\", subclause)\n",
    "#             print(check_verb_subject_in_clause(subclause))\n",
    "            if check_verb_subject_in_clause(subclause):\n",
    "                clause_list.append(subclause)\n",
    "    \n",
    "    return clause_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sentence = en_data[889]\n",
    "sentence0 = 'The sky is blue.'\n",
    "sentence1 = \"the city of the loir-et-cher is part of the greater mumbai\"\n",
    "sentence2 = 'Even with the weather being that nasty, the couple and their families decided to go ahead \\\n",
    "with the wedding as planned.'\n",
    "sentence3 = 'My little daughter loves to play with her dolls.'\n",
    "sentence4 = 'Because she loves her students, Mrs Stevens will be sad on the last day of school.'\n",
    "sentence5 = 'My mother suggested that I should consult a doctor'\n",
    "sentence6 = 'In the spring, Damien will run his first marathon.'\n",
    "sentence7 = 'If there is no leftover pizza, Rosa usually eats \\\n",
    "whole-grain cereal.'\n",
    "sentence8 = 'Huffing and puffing, we arrived at the classroom door with only seven seconds to spare'\n",
    "compound_sentence = 'I like road bikes, and he likes mountain bikes.'\n",
    "compound_sentence = 'She ran with the dogs, I swam with the fishes, and they biked to the mountains.'\n",
    "complex_sent = 'John retired when he turned 65'\n",
    "complex_sent2 = 'Whether you agree with me or not makes little \\\n",
    "difference to our inverstors, who by the way, are the ones most \\\n",
    "affected by whatever mistake we make.'\n",
    "complex_sent3 = 'Whoever thought of the idea is a genius.'\n",
    "complex_compound1 = \"Bill voted against the measure \\\n",
    "because he felt that it wasn't strong enough, but he also offered \\\n",
    "to continue discussions, which we will do next week.\" \n",
    "\n",
    "\n",
    "current = sentence8\n",
    "doc = nlp(current)\n",
    "print(\"The sentence :\", current,\" \\n\\n\")\n",
    "\n",
    "print(\"Its analysis :\")\n",
    "for token in doc:\n",
    "    print(token.text,\" | \",  token.dep_,\" | \", token.pos_, \" | \", token.head.text,\" | \",  token.head.pos_,\" | \", \n",
    "           [left.text for left in token.lefts], \" | \",\n",
    "           [right.text for right in token.rights], \" | \",\n",
    "           [child.text for child in token.children])\n",
    "   \n",
    "print(\"The extracted clauses :\")\n",
    "clauses = extract_clauses(current)\n",
    "for clause in clauses:\n",
    "    print(clause)\n",
    "displacy.serve(doc, style='dep')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence : In the spring, Damien will run his first marathon.  \n",
      "\n",
      "\n",
      "The extracted clauses :\n",
      "Clause : [In, the, spring, ,, Damien, will, run, his, first, marathon, .]\n",
      "(['Damien', 'Damien'], ['the spring', 'his first marathon'], [run, run], ['pobj', 'dobj'])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "entity and relations from the blog:\n",
      "Subject : Damien | Object : first  marathon | Relation : run\n",
      "\n",
      "\n",
      "\n",
      "entity and relations from my code:\n",
      "Subject : Damien | Object : the spring | Relation : run\n",
      "\n",
      "\n",
      "\n",
      "entity and relations from my code:\n",
      "Subject : Damien | Object : his first marathon | Relation : run\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/runpy.py:193: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"2c67f6d41be3419cb964a78a48a5e159-0\" class=\"displacy\" width=\"1625\" height=\"487.0\" direction=\"ltr\" style=\"max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">In</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">spring,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">Damien</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">will</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">run</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">his</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">first</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">marathon.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2c67f6d41be3419cb964a78a48a5e159-0-0\" stroke-width=\"2px\" d=\"M70,352.0 C70,2.0 925.0,2.0 925.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2c67f6d41be3419cb964a78a48a5e159-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,354.0 L62,342.0 78,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2c67f6d41be3419cb964a78a48a5e159-0-1\" stroke-width=\"2px\" d=\"M245,352.0 C245,264.5 385.0,264.5 385.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2c67f6d41be3419cb964a78a48a5e159-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,354.0 L237,342.0 253,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2c67f6d41be3419cb964a78a48a5e159-0-2\" stroke-width=\"2px\" d=\"M70,352.0 C70,177.0 390.0,177.0 390.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2c67f6d41be3419cb964a78a48a5e159-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M390.0,354.0 L398.0,342.0 382.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2c67f6d41be3419cb964a78a48a5e159-0-3\" stroke-width=\"2px\" d=\"M595,352.0 C595,177.0 915.0,177.0 915.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2c67f6d41be3419cb964a78a48a5e159-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,354.0 L587,342.0 603,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2c67f6d41be3419cb964a78a48a5e159-0-4\" stroke-width=\"2px\" d=\"M770,352.0 C770,264.5 910.0,264.5 910.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2c67f6d41be3419cb964a78a48a5e159-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,354.0 L762,342.0 778,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2c67f6d41be3419cb964a78a48a5e159-0-5\" stroke-width=\"2px\" d=\"M1120,352.0 C1120,177.0 1440.0,177.0 1440.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2c67f6d41be3419cb964a78a48a5e159-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,354.0 L1112,342.0 1128,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2c67f6d41be3419cb964a78a48a5e159-0-6\" stroke-width=\"2px\" d=\"M1295,352.0 C1295,264.5 1435.0,264.5 1435.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2c67f6d41be3419cb964a78a48a5e159-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1295,354.0 L1287,342.0 1303,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2c67f6d41be3419cb964a78a48a5e159-0-7\" stroke-width=\"2px\" d=\"M945,352.0 C945,89.5 1445.0,89.5 1445.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2c67f6d41be3419cb964a78a48a5e159-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1445.0,354.0 L1453.0,342.0 1437.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'dep' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#my code for extracting entities and relations\n",
    "\n",
    "def extract_subject_object(sent):\n",
    "    doc = nlp(sent)\n",
    "    subject = []\n",
    "    obj = []\n",
    "    for token in doc:\n",
    "        \n",
    "        if token.dep_.find(\"subj\") == True and len(subject) == 0:\n",
    "            for child in token.children:\n",
    "                if child.dep_ not in [\"det\", \"punct\", \"conj\", \"prep\"]:\n",
    "                    subject.append(child.text)\n",
    "            subject.append(token.text)\n",
    "        \n",
    "        if token.dep_.find(\"dobj\") == True and len(obj) == 0:\n",
    "            for child in token.children:\n",
    "                if child.dep_ not in [\"det\", \"punct\", \"conj\", \"prep\"]:\n",
    "                    obj.append(child.text)\n",
    "            obj.append(token.text)\n",
    "    \n",
    "    subject_string = \"\"\n",
    "    for token in subject:\n",
    "        subject_string += token\n",
    "        subject_string += \" \"\n",
    "    sub_string = subject_string.strip()\n",
    "    \n",
    "    object_string = \"\"\n",
    "    for token in obj:\n",
    "        object_string += token\n",
    "        object_string += \" \"\n",
    "    obj_string = object_string.strip(\" \")\n",
    "    return (sub_string, obj_string)\n",
    "\n",
    "\n",
    "def extract_entities_from_clause(token_list):\n",
    "    '''\n",
    "   Given a clause in the form of a token list (to preserve the dependency of the \n",
    "   tokens from the original sentence) returns the possible entities(subject, object). In the \n",
    "   absence of an object it looks for an appropriate substitute e.g. adjective or something.\n",
    "    '''\n",
    "    root_children = []\n",
    "    object_entity_list = []\n",
    "    subject_entity = ''\n",
    "    subject_entity_list = []\n",
    "    entity2_dep_list = []\n",
    "    intrans_verb_dep_list = [\"prep\", \"acomp\", \"advmod\", \"attr\"]\n",
    "    for token in token_list:\n",
    "        \n",
    "        if token.dep_.find(\"obj\") == True:\n",
    "            entity, entity_string, ent_dep = get_subtree_excluding_subclause(token)\n",
    "            object_entity_list.append(entity_string)\n",
    "            #print(\" The object : \",object_entity_list)\n",
    "            entity2_dep_list.append(token.dep_)\n",
    "\n",
    "        if token.dep_.find(\"subj\") == True:\n",
    "            sub_entity, sub_entity_string, sub_ent_dep = get_subtree_excluding_subclause(token)\n",
    "            subject_entity = sub_entity_string\n",
    "            #print(\" The subject :\", get_subtree_excluding_subclause(token))\n",
    "    #if there are no objects look for the following:\n",
    "    #look for adjectives?\n",
    "    relation_token = extract_relation_from_clause(token_list)\n",
    "    if relation_token is not None:\n",
    "        relation_subtree = relation_token.subtree\n",
    "    else:\n",
    "        relation_subtree = []\n",
    "    if len(object_entity_list) == 0:\n",
    "        print('No object, so looking for something else')\n",
    "        for token in relation_subtree:\n",
    "            if token.dep_ in intrans_verb_dep_list:\n",
    "                entity, entity_string, ent_dep = get_subtree_excluding_subclause(token)\n",
    "                object_entity_list.append(entity_string)                \n",
    "                #print(\" The object : \",get_subtree_excluding_subclause(token))\n",
    "                entity2_dep_list.append(token.dep_)\n",
    "    \n",
    "    \n",
    "    subject_entity_list = [subject_entity for i in range(len(object_entity_list))]\n",
    "    relation_list = [relation_token for i in range(len(object_entity_list))]   \n",
    "    \n",
    "    return subject_entity_list, object_entity_list, relation_list, entity2_dep_list\n",
    "\n",
    "\n",
    "def extract_relation_from_clause(token_list):\n",
    "    '''\n",
    "    Given a clause in the form of a token list (to preserve the dependency of the \n",
    "    tokens from the original sentence) returns the \n",
    "    possible relation (verb). The relation is calculated \n",
    "    by taking the verb and \n",
    "    \n",
    "    \n",
    "    input:\n",
    "        clause : a string containing a single clause\n",
    "    '''\n",
    "    \n",
    "    #if root is verb select that.\n",
    "    \n",
    "    for token in token_list:\n",
    "        #pdb.set_trace()\n",
    "        if token.dep_ in ['ROOT']:\n",
    "            return token\n",
    "    \n",
    "    #if not, look for verb\n",
    "    for token in token_list:\n",
    "        if token.pos_ in ['VERB']:\n",
    "            return token\n",
    "        \n",
    "    #if not, look for auxillary\n",
    "    for token in token_list:\n",
    "        \n",
    "        if token.dep_ in ['aux']:\n",
    "            return token\n",
    "    \n",
    "    \n",
    "            \n",
    "            \n",
    "def extract_entity_relation_tuples(sent):\n",
    "    '''\n",
    "    Given a sentence, returns a list of entity relation tuples\n",
    "    Entity relation tuples are extracted for each of the clauses\n",
    "    detected in the sentence.\n",
    "    input:\n",
    "        sent : A sentence in the form of a string.\n",
    "    \n",
    "    output:\n",
    "        \n",
    "        entity_rel_list : A list containing enitiy relations extracted from \n",
    "                          individual clause.\n",
    "                          of the form [((subj, obj), rel), (), ()]\n",
    "    '''\n",
    "    clause_list = extract_clauses(sent)\n",
    "    entity_rel_list = []\n",
    "    for clause in clause_list:\n",
    "        \n",
    "        sub_list, obj_list, rel_list, _ = extract_entities_from_clause(clause)\n",
    "        \n",
    "        for i in range(len(sub_list)):\n",
    "            entity_rel_list.append(((sub_list[i], obj_list[i]), rel_list[i]))\n",
    "    \n",
    "    \n",
    "    return entity_rel_list\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "sentence = en_data[8]\n",
    "sentence0 = 'The sky is blue.'\n",
    "sentence1 = \"the city of the loir-et-cher is part of the greater mumbai\"\n",
    "sentence2 = 'Even with the weather being that nasty, the couple and their families decided to go ahead \\\n",
    "with the wedding as planned.'\n",
    "sentence3 = 'My little daughter loves to play with her dolls.'\n",
    "sentence4 = 'Because she loves her students, Mrs Stevens will be sad on the last day of school.'\n",
    "sentence5 = 'My mother suggested that I should consult a doctor'\n",
    "sentence6 = 'In the spring, Damien will run his first marathon.'\n",
    "sentence7 = 'If there is no leftover pizza, Rosa usually eats \\\n",
    "whole-grain cereal.'\n",
    "sentence8 = 'Huffing and puffing, we arrived at the classroom door with only seven seconds to spare'\n",
    "compound_sentence = 'I like road bikes, and he likes mountain bikes.'\n",
    "compound_sentence = 'She ran with the dogs, I swam with the fishes, and they biked to the mountains.'\n",
    "complex_sent = 'John retired when he turned 65'\n",
    "complex_sent2 = 'Whether you agree with me or not makes little \\\n",
    "difference to our inverstors, who by the way, are the ones most \\\n",
    "affected by whatever mistake we make.'\n",
    "complex_sent3 = 'Whoever thought of the idea is a genius.'\n",
    "complex_compound1 = \"Bill voted against the measure \\\n",
    "because he felt that it wasn't strong enough, but he also offered \\\n",
    "to continue discussions, which we will do next week.\" \n",
    "\n",
    "\n",
    "current = sentence6\n",
    "doc = nlp(current)\n",
    "print(\"The sentence :\", current,\" \\n\\n\")\n",
    " \n",
    "\n",
    "print(\"The extracted clauses :\")\n",
    "clauses = extract_clauses(current)\n",
    "\n",
    "for clause in clauses:\n",
    "    print(\"Clause :\", clause)\n",
    "    print(extract_entities_from_clause(clause))\n",
    "    \n",
    "print(\"\\n\\n\")\n",
    "#getting entity relations from the blog\n",
    "entities_other = get_entities(current)\n",
    "relation_other = get_relation(current)\n",
    "print(\"\\nentity and relations from the blog:\\nSubject : {} | Object : {} | Relation : {}\\n\\n\".format(entities_other[0], \n",
    "                                                                               entities_other[1],\n",
    "                                                                               relation_other))\n",
    "\n",
    "#getting entity relations my code\n",
    "entity_list = extract_entity_relation_tuples(current)\n",
    "for entity in entity_list:\n",
    "    subj, obj = entity[0]\n",
    "    rel = entity[1]\n",
    "    print(\"\\nentity and relations from my code:\\nSubject : {} | Object : {} | Relation : {}\\n\\n\".format(subj, \n",
    "                                                                              obj, rel))\n",
    "\n",
    "#print(extract_entities_from_clause(current))\n",
    "#print(extract_relation_from_clause(current))\n",
    "\n",
    "displacy.serve(doc, style='dep')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyRL3",
   "language": "python",
   "name": "pyrl3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
